#!/bin/ksh93
# this script is the successor to the r.mpirun... series of scripts with expanded functionality
# (stdout/stderr post processing and "launch in background" capability)
#
# commands used in the highly parallel area, we want to use the actual path
# to avoid undue searches through the command path (involving shared filesystems)
# this is highly desirable when hundreds of MPI processes are fired
c_rm=$(which rm)
c_date=$(which date)
c_cat=$(which cat)
c_sleep=$(which sleep)
c_true=$(which true)
c_sed=$(which sed)
c_env=$(which env)
c_wc=$(which wc)
c_touch=$(which touch)
c_sort=$(which sort)
c_hostname=$(which hostname)
[[ "$1" == --version ]] && set -- -version
# CAVEAT: this script uses typeset -Z (does not work for the bash shell, only for ksh93)
typeset -Z4 SeqNum MemberNo MemberChild
typeset -A nodecore
#################################################################################################################################
list_message()
{
  [[ -n ${processorder} ]] || return
  echo "INFO: =============================================="
  echo "INFO: temporary listings for all members in ${tmpdir}"
  echo "INFO: =============================================="
}
#################################################################################################################################
#    node_to_rank ${MY_NODEFILE} ${ThreadsInWorld[${MpiCommWorld}]}
node_to_rank() {
#  echo "INFO: MaxCores=${MaxCores}" 1>&2
  ((r=0))
  ((rr=-1))
  Increment=${2:-1}
  Thinning=${3:-${Increment}}
  rm -f ${1}.thinned
  for Target in $(cat ${1}) ; do
    ((rr=rr+1))
    if ((rr%Thinning!=0)) ; then continue ; fi
    echo "$Target" >>${1}.thinned
    BaseCore=${nodecore[$Target]}
    if ((MaxCores <  BaseCore+Increment)) ; then
      echo "ERROR in world ${MpiCommWorld}: cores available = $MaxCores, cores needed = $((BaseCore+Increment))" 1>&2
      return 1
    fi
    if ((Increment>1)) ; then
      echo "rank $r=$Target  slot=${BaseCore}-$((BaseCore+Increment-1))"
#      echo "rank $r=$Target  slot=${nodecore[$Target]}-$((nodecore[$Target]+Increment-1))" 1>&2
    else
      echo "rank $r=$Target  slot=${BaseCore}"
#      echo "rank $r=$Target  slot=${nodecore[$Target]}" 1>&2
    fi
    ((nodecore[$Target]=nodecore[$Target]+Increment))
    ((r=r+1))
  done
  mv ${1}.thinned ${1}
}
#################################################################################################################################
# MPI prelauncher for Linux systems using mpich2 or openmpi. takes care of most OpemMPI/Mpich2 issues
mpiexec_Linux() {
#  RANK_VAR=$(${mpirun} -n 1 /usr/bin/env | grep RANK | grep -E "OMPI_COMM_WORLD_RANK|PMI_RANK")
  RANK_VAR=OMPI_COMM_WORLD_RANK
  which orted >/dev/null || RANK_VAR=PMI_RANK
  case "$RANK_VAR" in
    OMPI_COMM_WORLD_RANK*)                                     # OpenMPI
      [[ "$bind" == bind ]] && \
        OPEN_MPI_PARMS="-bind-to-core ${OPEN_MPI_PARMS}"
      [[ "$bind" == none ]] && \
        OPEN_MPI_PARMS="-bind-to-none ${OPEN_MPI_PARMS}"
      [[ -n "$ib" ]] && \
        OPEN_MPI_PARMS="--mca btl openib,tcp,self ${OPEN_MPI_PARMS}"  # force use of infiniband
      [[ -n "$noib" ]] && \
        OPEN_MPI_PARMS="--mca btl tcp,self ${OPEN_MPI_PARMS}"         # do not try to use infiniband
      [[ "$mpirun" == mpirun ]] && [[ -n ${OPAL_PREFIX} ]] && \
        OPEN_MPI_PARMS="--prefix ${OPAL_PREFIX} ${OPEN_MPI_PARMS}"    # if not using rumpirun.openmpi (Dorval EC clusters)
      OPEN_MPI_PARMS="--mca shmem posix ${OPEN_MPI_PARMS}"
      echo "=== OPEN_MPI_PARMS=${OPEN_MPI_PARMS} ==="
      mpiexec_Linux_launch OpenMPI ${OPEN_MPI_PARMS} "$@"
      ;;
    PMI_RANK*)                                                 # mpich2 family
      echo "ERROR: (${0}) mpich2 Linux MPI launcher not implemented yet"
      MPICH_PARMS="-envall"
      return 1
      mpiexec_Linux_launch Mpich2 "${MPICH_PARMS}" "$@"
      ;;
    *)
      echo "ERROR: (${0}) RANK_VAR=$RANK_VAR "
      echo "ERROR: (${0}) Linux MPI launcher other than OpenMPI or mpich2 not implemented yet "
      return 1
      ;;
  esac
  return
}
#################################################################################################################################
# MPI launcher for Linux systems (
# tested with openmpi interactively and under GridEngine
#
mpiexec_Linux_launch() {
  Launcher="$1"
  LaunchStatus=""
  shift
#
  ((npe_total=TotalInstances))
#  ((npe_total=TotalThreads)) # not ready yet
#  make_node_file
#
# we are assuming that mpirun/mpiexec can be backgrounded.
# should this assumption be false the following line must be uncommented
#  ((MpiCommWorlds>1)) && echo "ERROR: MpiCommWorlds>1 not supported yet" && return
#
  unset PE_HOSTFILE                                                             # Dorval EC clusters
#  [[ -f "${GECOSHEP_HOSTS_FILE}" ]] && export OMPI_MCA_orte_rsh_agent=rurun     # Dorval EC clusters
#  export OMPI_MCA_plm_rsh_disable_qrsh=1
#
  export MpiCommWorld=0
  export ChildOffset=0
  export WorldOffset=0
  HostOffset=0
  for i in $(uniq ${MY_NODEFILE}) ; do  # start core table at 0 for all nodes
    nodecore[$i]=0
  done
  MY_NODEFILE_ORI=${MY_NODEFILE}
  echo "INFO: using : mpirun = '${mpirun}'"
  while(( MpiCommWorld<MpiCommWorlds)) ; do   # loop over MPI worlds
#
    echo "INFO: MPI world ${MpiCommWorld} will be using ${PeInWorld[${MpiCommWorld}]} tasks  with ${ThreadsInWorld[${MpiCommWorld}]} thread(s) per task"
    ((FirstHost=HostOffset+1))
#    if [[ "${nodemap}" == NoNe ]] ; then    # one entry per thread in nodefile, thinning needed
#      ((HostOffset=HostOffset+${PeInWorld[${MpiCommWorld}]}*${ThreadsInWorld[${MpiCommWorld}]})) ; ThinBy=${ThreadsInWorld[${MpiCommWorld}]}
#    else                                    # one entry per process in nodefile
      ((HostOffset=HostOffset+${PeInWorld[${MpiCommWorld}]})) ; ThinBy=1
#    fi
    export MY_NODEFILE=${MY_NODEFILE_ORI}.${MpiCommWorld}
    export MY_RANKFILE=${MY_NODEFILE_ORI}.rank.${MpiCommWorld}
    sed -n ${FirstHost},${HostOffset}p ${MY_NODEFILE_ORI} >${MY_NODEFILE}
#   transformer ${MY_NODEFILE} en map file pour OpenMPI et en host file pour Mpich2
    node_to_rank ${MY_NODEFILE} ${ThreadsInWorld[${MpiCommWorld}]} ${ThinBy} > ${MY_RANKFILE} || { LaunchStatus="$LaunchStatus ${MpiCommWorld}" ; break; }
#    cat ${MY_RANKFILE}
    echo "INFO: ===== $(cat ${MY_NODEFILE} | wc -l) host(s) in  ${Launcher} World $MpiCommWorld ====="
    cat ${MY_NODEFILE}
    list_message
    EXTRA_MPI_PARMS=""
    if [[ $Launcher == OpenMPI ]] ; then #   pass environment variables (only names starting with letter or digit) (only for OpenMPI)
#      EXTRA_MPI_PARMS="-cpus-per-proc ${ThreadsInWorld[${MpiCommWorld}]}"
      EXTRA_MPI_PARMS="${EXTRA_MPI_PARMS} $(s.prefix '-x ' $(env | grep -v "'" | grep -v '"'  | grep  '^[a-zA-Z0-9]' | sed 's/=.*//' | sort ))"
#     ignore rank file in interactive case
      tty -s || EXTRA_MPI_PARMS="${EXTRA_MPI_PARMS} --rankfile ${MY_RANKFILE}"
#      echo "INFO: RANK file for ${Launcher}  world $MpiCommWorld"
#      cat ${MY_RANKFILE}
    fi
#
    if ((MpiCommWorlds>1)) ; then   # more than one MPI world, launch mpirun in background
      echo "INFO: ==== Backgrounding ${Launcher} world ${MpiCommWorld} with ${PeInWorld[${MpiCommWorld}]} MPI tasks ===="
#      set -x
      [[ -z ${dryrun} ]] && \
        ${mpirun} ${EXTRA_MPI_PARMS} -machinefile ${MY_NODEFILE} -n ${PeInWorld[${MpiCommWorld}]} "$@" ${ParallelScript}.${MpiCommWorld} &
      set +x
    else   # only one MPI world, execute mpirun
      echo "INFO: ==== Launching ${Launcher} world ${MpiCommWorld} with ${PeInWorld[${MpiCommWorld}]} MPI tasks ===="
      [[ -n ${verbose} ]] && \
        echo "INFO: ${mpirun} ${EXTRA_MPI_PARMS} -machinefile ${MY_NODEFILE} -n ${PeInWorld[${MpiCommWorld}]} "$@" ${ParallelScript}.${MpiCommWorld}"
#      set -x
      [[ -z ${dryrun} ]] && \
        ${mpirun} ${EXTRA_MPI_PARMS} -machinefile ${MY_NODEFILE} -n ${PeInWorld[${MpiCommWorld}]} "$@" ${ParallelScript}.${MpiCommWorld}
      set +x
    fi
    PeAdded=${PeInWorld[${MpiCommWorld}]}
    ((ChildOffset=ChildOffset+PeAdded))
    ((WorldOffset=ChildOffset))
    ((MpiCommWorld=MpiCommWorld+1))
  done
  if ((MpiCommWorlds>1)) ; then
    echo "==== Waiting for ${MpiCommWorlds} ${Launcher} worlds to terminate ===="
    wait
  fi
  if [[ -n $LaunchStatus ]] ; then
    echo "###########################################################################################"
    echo "#   ERRORS detected during launch in world $LaunchStatus, some process(es) will be missing           #"
    echo "###########################################################################################"
    return 1
  fi
  return 0
#
}
#################################################################################################################################
#
# MPI launcher for AIX, using POE under LoadLeveler, uses MP_NEWJOB for multiple MPI worlds
#
mpiexec_AIX() {
#
  ((npe_total=TotalInstances))
  make_node_file
#  ((MpiCommWorlds>1)) && echo "ERROR: MpiCommWorlds>1 not supported yet" && return
  export ENVIRONMENT_PASSED=yes
  export MpiCommWorld=0
  export ChildOffset=0
  export WorldOffset=0
  if tty -s ; then  # node list only necessary in interactive case
    echo "INFO: interactive use of MPI on AIX"
    export MP_PROCS
    export MP_HOSTFILE=${MY_NODEFILE}
    grep -q $(hostname) ${HOME}/.rhosts 2>/dev/null || echo $(hostname) ${USER} >>${HOME}/.rhosts
    chmod 700 ${HOME}/.rhosts
  fi
  list_message
  if((MpiCommWorlds==1 && ${npe_total} == ${BATCH_MPI_CPUS:-npe_total} )) ; then  # only one world, use regular call to poe
    ((MP_PROCS=${PeInWorld[0]}))
    echo "========== poe command file with ${MP_PROCS} tasks in single MPI world =========="
    [[ -z ${dryrun} ]] && poe ${ParallelScript}.0 </dev/null
  else
    if [[ "${LOADL_STEP_CLASS}" == "preemptable" ]] ; then
      echo "multiple MPI worlds not supported for preemptable jobs"
      exit 1
    fi
    cat <<EOT >${tmpdir}/null_mpi.c
#include <mpi.h>
void main(int argc, char **argv)
{
 MPI_Init(&argc,&argv);
 MPI_Finalize();
}
EOT
    mpcc -o ${tmpdir}/null_mpi ${tmpdir}/null_mpi.c
    rm -f ${ParallelScript}.ALL
    ((MP_PROCS=0))
    while(( MpiCommWorld<MpiCommWorlds)) ; do  # build POE command file((MpiCommWorlds=MpiCommWorlds+1))
      echo "${ParallelScript}.${MpiCommWorld}@$((MpiCommWorld+1))%${PeInWorld[${MpiCommWorld}]}%mpi:*" >>${ParallelScript}.ALL
      ((MP_PROCS=MP_PROCS+${PeInWorld[${MpiCommWorld}]}))
      ((MpiCommWorld=MpiCommWorld+1))
    done
    export MP_NEWJOB=parallel
    export ChildOffset=0   # poe takes care of adjusting MP_CHILD
    echo MP_PROCS=${MP_PROCS}
    echo "COMPLETE" >>${ParallelScript}.ALL # terminate POE command file
    echo "========== poe command file with ${MP_PROCS} tasks in ${MpiCommWorlds} MPI world(s) =========="
    cat ${ParallelScript}.ALL
    # multiple poe instances in parallel, gather, consolidate, run
    [[ -z ${dryrun} ]] && poe -cmdfile ${ParallelScript}.ALL </dev/null
  fi
}
#################################################################################################################################
#
# print list of failed tasks
# clean up tmpdir directory used for launch help files and listings
#
ListFailed() {
  ls ${tmpdir}/fail.*  2>/dev/null 1>/dev/null || return 0    # if no process failure flag file found
  if [[ "${e}" == YES ]] ; then
    echo "ERROR: some processes have failed during execution"
    (cd ${tmpdir} ; ls -1 | grep 'fail[.]' | sed 's/fail.//' | xargs -l5 /bin/echo  )   # list, 5 per line
  fi
}
# ===========================================================================
#
# final cleanup and set exit status
#
local_cleanup() {
 ExitStatus=0
 ls  ${tmpdir}/fail.* 2>/dev/null 1>/dev/null && echo "INFO: RUN FAILED" && echo "INFO: first 10 failing processes :" && ExitStatus=1
 ls  ${tmpdir}/fail.* 2>/dev/null | head -10 | sed "s:${tmpdir}/::g" | xargs -l5 echo
 [[ ${nocleanup} == on_error && $ExitStatus == 0 ]] && nocleanup=""  # -nocleanup on_error , unset nocleanup if no error detected
 [[ -n ${nocleanup} ]] && return $ExitStatus  # -nocleanup option used, return
 rm -rf ${tmpdir} || ( sleep 1 ; rm -rf ${tmpdir} ) &   # cannot clean tmpdir while script runs, use delayed remove
 return $ExitStatus
}
# ===========================================================================
#
# print inter task listing separator
#
print_separator() {
  [[ -n ${nosep} ]] && return   # -nosep option used, return
  echo "==============      $@      =============="
}
# ===========================================================================
#
# default script for post-processing of process listing files
# stdout/stderr from each task listed in order
# line tagging already done
#
cat_output() {  #  cat captured stdout/stderr files into stdout with appropriate tagging
  OutputFound=""
  for OUTDIR in $* ; do
    [[ -r ${OUTDIR}/stdout ]] && cat ${OUTDIR}/stdout && OutputFound="yes"
  done
  [[ ${OutputFound} == yes ]] && return
# Output preprocessing within MPI was not done for some reason, process outputs the hard way
  for the_file in ${tmpdir}/[0-9][0-9][0-9][0-9]/stdout.*
  do
    MP_Tag=${the_file##*.}
    [[ -f ${the_file} ]] && \
    cat ${the_file} | sed "s/^/${Prefix2}${MP_Tag}: /"
    [[ -f ${the_file%/*}/stderr ]] && \
    echo "${Prefix3}${MP_Tag}: ============== stderr ${MP_Tag} ==============" && \
    cat ${the_file%/*}/stderr | sed "s/^/${Prefix3}${MP_Tag}: /"
  done
}
#################################################################################################################################
#
# create the MPI C and Fortran executables used in the self test
# these programs will be built to fail for process N (N may be higher than actual number of PEs for the call)
#
make_cf_test() {
[[ ${selftest} == *.fail.* ]] && selftest_fail=${selftest##*fail.}
selftest_fail=${selftest_fail:--1}
[[ ${selftest} == *fail* ]] && echo "INFO: test program(s) will be configured to fail in process ${selftest_fail}"
echo "INFO: creating C test program source mpi_c_test.c"
#
cat >${tmpdir}/mpi_c_test.c <<EOT
#include <unistd.h>
#include <stdlib.h>
#include <stdio.h>
#include <mpi.h>

void main(int argc, char **argv)
{
 int my_rank=-1;
 char hostname[1204];

 gethostname(hostname, 1023);
 MPI_Init(&argc,&argv);
 MPI_Comm_rank(MPI_COMM_WORLD , &my_rank);
 printf("host = %s, C process rank = %d \n",hostname,my_rank);
 if(my_rank==${selftest_fail}) {
   printf("process %d failing\n",my_rank);
   exit(1);
 }
 MPI_Finalize();
}
EOT
echo "INFO: creating Fortran test program source mpi_f_test.f90"
cat >${tmpdir}/mpi_f_test.f90 <<EOT
program demo
implicit none
include 'mpif.h'
integer :: ierr,myrank
call mpi_init(ierr)
call mpi_comm_rank(MPI_COMM_WORLD,myrank,ierr)
if(myrank==${selftest_fail}) then
  write(6,*)"FORTRAN process no",myrank," failing"
  call wrong(0.0)
endif
write(6,*)"FORTRAN process no",myrank," running"
call mpi_barrier(MPI_COMM_WORLD,ierr)
call mpi_finalize(ierr)
stop
end
subroutine wrong(div)
call div(0)
end
EOT
#
echo INFO: compiling C test program mpi_c_test.c into mpi_c_test
which mpicc 2>/dev/null 1>/dev/null && \
  mpicc -o ${tmpdir}/mpi_c_test ${tmpdir}/mpi_c_test.c
which mpcc 2>/dev/null 1>/dev/null && \
  mpcc -o ${tmpdir}/mpi_c_test ${tmpdir}/mpi_c_test.c
echo INFO: removing C test program source mpi_c_test.c
rm ${tmpdir}/mpi_c_test.c
#
echo INFO: compiling Fortran test program mpi_f_test.f90 into mpi_f_test
which mpif90 2>/dev/null 1>/dev/null && \
  mpif90 ${tmpdir}/mpi_f_test.f90 -mp -o ${tmpdir}/mpi_f_test 2>/dev/null || \
  mpif90 ${tmpdir}/mpi_f_test.f90 -o ${tmpdir}/mpi_f_test
which mpxlf90_r 2>/dev/null 1>/dev/null && \
  mpxlf90_r ${tmpdir}/mpi_f_test.f90 -o ${tmpdir}/mpi_f_test
[[ -x ${tmpdir}/mpi_f_test ]] && echo INFO: mpi_f_test created
[[ -x ${tmpdir}/mpi_c_test ]] && echo INFO: mpi_c_test created
echo INFO: removing Fortran test source program mpi_f_test.f90
rm ${tmpdir}/mpi_f_test.f90
}
#################################################################################################################################
# process  -geometry option
expand_geometry_map() {
  echo -geometry option not implemented yet
  return 1
#
  if [[ -f "${PARALLEL_NODEFILE}" ]] ; then  # PARALLEL_NODEFILE used, override computed node list
    cp ${PARALLEL_NODEFILE} ${MY_NODEFILE}
  fi
}
#################################################################################################################################
# process  -nodemap option
#          -nodemap s1,e1:np1 s2,e2:np2 ... sN,eN:npN
#          si : first host, ei : last host, npi : number of PEs on node
#          (alternate form : si,di,ei:npi , npi PEs on hosts si thru ei every di )
#          (alternate form : si:npi , npi PEs on host si, equivalent to si,si:npi or si,1,si:npi )
# example: -nodemap 0:1 5:3 6,2,10:4     (1 PE on host 0, 3PEs on host 5, 4 PEs on hosts 6, 8, 10 )
expand_node_map() {
  [[ "$hostos" == Linux ]] || return   # Linux only
  [[ "${coremap}" == NoNe ]] || return # mutually exclusive options coremap/nodemap

  [[ "${nodemap}" == @* ]] && [[ -r "${nodemap#@}" ]] && nodemap="$(cat ${nodemap#@})"   # -nodemap @readable_file
# expand_node_map (expand a series of number doublets/triplets into a full list)
  ListOfHosts=($(uniq ${nodefile}))
  NumberOfHosts=${#ListOfHosts[@]}
  for Entry in ${nodemap} ; do
    npe0=${Entry#*:}
    hlist="$(echo ${Entry%:*} | tr , ' ')"                          # replace , with space for seq
    [[ "${Entry%:*}" != *,* ]] && hlist="${Entry%:*} ${Entry%:*}"   # single number si:npi
    for hnum in $(seq ${hlist}) ; do
      ((npe=npe0))
      tty -s && ((hnum=hnum%NumberOfHosts))    # number of hosts is phony (often 1) if interactive
      while ((npe>0)) ; do echo ${ListOfHosts[$((hnum))]} >>${MY_NODEFILE} ; ((npe=npe-1)) ;done
    done
  done
}
#################################################################################################################################
# process  -coremap option
#          -coremap series of 'start,increment,end' triplets or 'start,end' doublets
#          -coremap @readable_file        (contents as above)
#
# example: -coremap 0,1,6 12,2,23   ( 0 thru 6 , 12 thru 23 by 2)
expand_core_map() {
  [[ "$hostos" == Linux ]] || return   # Linux only
  [[ "${nodemap}" == NoNe ]] || return # mutually exclusive options coremap/nodemap

  [[ "${coremap}" == @* ]] && [[ -r "${coremap#@}" ]] && coremap="$(cat ${coremap#@})"   # -coremap @readable_file
  coremap=${coremap:-0,${OMP_NUM_THREADS},$((TotalThreads-1))}        # by default, use all threads from all PEs in order
# expand_core_map (expand a series of number doublets/triplets into a full list)
  CoreMapArray=( $(for i in ${coremap} ; do ii="${i}" ; [[ "${ii}" == 0 ]] && ii="0 0" ; seq $(echo ${ii} | tr , ' ') ; done) )
  NumberOfCores=${#CoreMapArray[@]}
  ListOfHosts=($(cat ${nodefile}))
  NumberOfHosts=${#ListOfHosts[@]}
  ((r=0))
  while ((r<NumberOfCores)) ; do
    h=${CoreMapArray[$r]}
    ((h>=NumberOfHosts)) && echo "ERROR: host number (${h}) > maximum ($((NumberOfHosts-1)))" && rm -rf ${tmpdir} && exit 1
    echo "${ListOfHosts[$h]}" >>${MY_NODEFILE}
    ((r=r+1))
  done
  [[ -n $verbose ]] && echo "INFO: Cores map entries = $NumberOfCores"
  [[ -n $verbose ]] && echo "INFO: Base core map = ${CoreMapArray[@]}"
}
#################################################################################################################################
# remap_nodes , uses node file and geometry file
# usage: remap_nodes node_list > new_reordered_node_list
#
remap_pass2() {
  ((Host=-1))
  while read Line
  do
    ((Host=Host+1))
    for i in $Line
    do
      echo $i ${HostList[$Host]} # task_number host_name
    done
  done
}
remap_nodes() {
  [[ -r "${1}" && -r "${geometry}" ]] || return 1
  ((Host=-1))
  for i in $(uniq ${1}) # build list of host names
  do
   ((Host=Host+1))
   HostList[$Host]=$i
  done
  cat ${geometry} | remap_pass2 | sort -n | cut '-d ' -f2
}
#################################################################################################################################
# prepare full node list (parts of it to be used by each MPI wolrd)
# -geometry processing will only work with openmpi/linux
# in that case a node file will be expected with as many nodes as there
# are lines in the geometry file
# under AIX the remapping is expected to be performed by the LoadLeveler
# geometry keyword in the job header
# all this will have to be revisited in the future in order to support
# heterogenous OpenMP factors (probably using the geometry)
# OMP_NUM_THREADS consistency with computed "loops" is not checked
# the script tries to keep the master node at the beginning of the node list
# this is why 'sort -u' has been replaced with 'uniq'
#
make_node_file() {   # this will be rewritten
  # if there is a geometry file, remap the node list file
  # remapping will fail if there is no node file pointed to by MY_NODEFILE
  ThreadCount=${1:-1}   # number of threads per process

  if [[ -z ${MY_NODEFILE} ]] ; then  # no node file has been found
    if tty -s ; then   # interactive, create a node file
      ((npe_temp=npe_total*ThreadCount))
      while (( npe_temp > 0 )) ; do ((npe_temp=npe_temp-1)) ; echo $(hostname) >> ${TMPDIR}/MY_NODEFILE ; done
    fi
  fi
#
  if [[ -f "${MY_NODEFILE}" ]] ; then  # node file exists
        rm -f $TMPDIR/MY_NODEFILE
        nhosts=$(sort -u ${MY_NODEFILE} | wc -l | sed -e 's/ //g')   # number of hosts
        ((loops=(npe_total+nhosts-1)/nhosts))   # number of tasks per host
        for i in $(uniq < $MY_NODEFILE) ; do
          for j in $(seq $loops) ; do
            echo ${i} >>${TMPDIR}/MY_NODEFILE   # one entry per task
          done
        done
  fi
#
  if [[ -f "${PARALLEL_NODEFILE}" ]] ; then  # PARALLEL_NODEFILE used, override computed node list
    cp ${PARALLEL_NODEFILE} $TMPDIR/MY_NODEFILE
  fi
#
  if [[ -f "${geometry}" && -z "${PARALLEL_NODEFILE}" ]] ; then   # task geometry and no overrride of node list
    mv $TMPDIR/MY_NODEFILE $TMPDIR/MY_NODEFILE.old
    remap_nodes $TMPDIR/MY_NODEFILE.old >$TMPDIR/MY_NODEFILE
    rm $TMPDIR/MY_NODEFILE.old
  fi
#
  export MY_NODEFILE=$TMPDIR/MY_NODEFILE
}
#################################################################################################################################
# wait_for creation|rm timeout directory file_names
# wait for file creation / removal with timeout (in seconds)
#
wait_for() {
  action=${1}
  time_out=${2}
  dir=${3}
  shift ; shift; shift
  for file in $* ; do
    while ((time_out>=0)); do
      [[ -f ${dir}/${file} ]]   && [[ ${action} == creation ]] && echo "INFO: ${dir}/${file} created" &&  break   # creation mode and file found
      [[ ! -f ${dir}/${file} ]] && [[ ${action} != creation ]] && echo "INFO: ${dir}/${file} deleted"  && break   # absence mode and file not found
      ((time_out=time_out-1))      # timeout decremented only if action failed
      sleep 1
    done
    ((time_out<=0)) && echo "ERROR: timeout waiting for ${action} of ${dir}/${file}" && return 1        # timeout expired, failure
  done
  return 0                          # timeout not expired, success
}
#################################################################################################################################
#                                       collect arguments
#################################################################################################################################
#
# MY_NODEFILE now contains the node list if one was specified before
# and will be the default value for -nodefile
#
MY_NODEFILE=${PARALLEL_NODEFILE:-${GECOSHEP_HOSTS_FILE:-${GECOSHEP_HOSTFILE:-${PBS_NODEFILE:-${LOADL_HOSTFILE}}}}}
Version="1.1.14 2014/12/16"
echo "${0##*/} version $Version avec /bin/ksh93"
eval `cclargs_lite -D "" $0 \
    -args "" "" "[arguments to the command]" \
    -bind "" "bind" "[]" \
    -coremap "NoNe" "NoNe" "[core affinity map]" \
    -debug "" "5" "[]" \
    -dryrun "" "dryrun" "[]" \
    -e "NO" "YES" "[list failed processes" \
    -errp "e" "E" "[stderr prefix in listings]" \
    -gdb "" "Default" "[directive file for gdb]" \
    -geometry "" "" "[]" \
    -ib "" "ib" "[use Infiniband]" \
    -hostos "$(uname -s)" "" "[local OS]" \
    -inorder "" "yes" "[list out/err of members in process order]" \
    -instancedir "${PARALLEL_INSTANCES_DIR}" "${PARALLEL_INSTANCES_DIR}" "[directory containing instance names]" \
    -instances "${PARALLEL_INSTANCES}" "${PARALLEL_INSTANCES}" "[instance name for member or list of instances for master]" \
    -map "" "map" "[display processor map]" \
    -maxcores "${MAX_CORES}" "65536" "[]" \
    -members "" "" "[NxM, N members of size M, override npey and npex]" \
    -minstdout "2" "2" "[]" \
    -mpiargs "" "" "[]" \
    -mpich2 "" "mpich2" "[use mpich2]" \
    -mpirun "$(which rumpirun.openmpi 2>/dev/null)" "mpirun" "[]" \
    -nocleanup "" "nocleanup" "[]" \
    -nodefile "${MY_NODEFILE}"  "${MY_NODEFILE}" "[list of nodes to use]" \
    -nodemap "NoNe" "NoNe" "[process to node map]" \
    -noib "" "noib" "[do not use Infiniband ]" \
    -nompi "run_with_mpi" "run_in_background" "[do not use mpi to launch]" \
    -nosep "" "yes" "[deactivate separator between members]" \
    -npex "${BATCH_MPI_CPUS:-1}" "${BATCH_MPI_CPUS:-1}" "[member size, total number of cpus if 1 member]" \
    -npey "1" "1" "[number of members]" \
    -offset "0" "1" "[numbering of members from this value]" \
    -openmpi "" "openmpi" "[use openmpi]" \
    -outp "o" "O" "[stdout prefix in listings]" \
    -packoutput "cat_output" "echo" "[]" \
    -pes "${BATCH_MPI_CPUS:-1}" "${BATCH_MPI_CPUS:-1}" "[number of PEs for ALL instances]" \
    -pgm "Invalid_Command.EXE" "" "[]" \
    -preexec "" "" "[prefix program execution with this (time/gdb/...)]" \
    -processorder "" "yes" "[list out/err of members in order]" \
    -selftest "" "c.f.fail.999999" "[quick selftest]" \
    -spliteo "no" "yes" "[split stderr from stdout]" \
    -tag "child" "full" "[full/child/member/stderr/none/seq]" \
    -timeout "60" "60" "[timeout for multiple instances]" \
    -tmpdir "$(pwd -P)/tmpdir${TRUE_HOST}$$" "" "[temporary directory visible by all processes]" \
    -verbose "" "verbose" "[]" \
    -version "" "${Version}" "[version number]" \
    ++ "$@" ${RUN_IN_PARALLEL_EXTRAS}`
#
[[ -n ${version} ]] && exit 0
# are we uisng gdb ?
if [[ -n $gdb ]] ; then
  [[ $gdb == Default ]] && preexec="$(which gdb) -batch -ex run -ex where"
  [[ -r $gdb ]] && preexec="$(which gdb) -batch -x $(true_path $gdb)"
fi
OMP_NUM_THREADS=${OMP_NUM_THREADS:-1}
#
CoresOnNode=128
[[ "$hostos" == AIX ]]&& CoresOnNode=$(($(bindprocessor -q | sed 's/.*[ ]//')+1))
[[ "$hostos" == Linux ]] && CoresOnNode=$(grep MHz /proc/cpuinfo 2>/dev/null | wc -l)   # Linux only
((CoresOnNode<=0)) && CoresOnNode=1
((MaxCores=CoresOnNode))
[[ -n $maxcores ]] && ((MaxCores=maxcores))
[[ "$TRUE_HOST" == eole* ]] && ib="ib"  # force usage of Infiniband on Dorval eole cluster
if [[ "$hostos" == AIX ]] ; then
  [[ "${nodemap}" != NoNe ]] && nodemap="" && echo "-nodemap not supported for AIX, use LoadLeveler geometry instead" && exit 1
  [[ "${coremap}" != NoNe ]] && coremap="" && echo "-coremap not supported for AIX, use LoadLeveler geometry instead" && exit 1
fi
#
if [[ "${mpich2}" == "mpich2" ]] ; then
  mpich2="$(which rumpirun.mpich2 2>/dev/null)"
  mpich2=${mpich2:-mpirun}
fi
if [[ "${openmpi}" == "openmpi" ]] ; then
  openmpi="$(which rumpirun.openmpi 2>/dev/null)"
  openmpi=${openmpi:-mpirun}
fi
mpirun=${openmpi:-${mpich2:-${mpirun:-mpirun}}}   # in case rumpirun.openmpi not found
#
echo "INFO: START of ${0##*/} : $(date)"
#[[ -z ${tmpdir} ]]  && tmpdir="$(pwd -P)/tmpdir${TRUE_HOST}$$"
[[ -n ${inorder} ]] && processorder="yes"
if [[ -n ${members} ]] ; then
  if [[ ${members} == *x* ]] ; then
    npex=${members#*x} ; npey=${members%x*}
  else
    npex=1 ; npey=${members}
  fi
fi
#
# number of MPI tasks
#
((npe_total=npex*npey))
((TotalInstances=npe_total))
((TotalTasks=npe_total))
#################################################################################################################################
#                                   do we have co-running master/slave instances ?
#################################################################################################################################
if [[ -d "${instancedir}" && -n "${instances}" ]] ; then     # we have co-running instances only if both are defined
  slaves=$(echo ${instances} | wc -w)   # if 1 it is a slave, if >1 it is the master
  if ((slaves>1)) ; then  # the master instance, there is MORE THAN ONE item  in instances
    echo "INFO: (master) START of ${slaves} slaves '${instances}' "
    wait_for creation ${timeout} ${instancedir} ${instances}   # wait for all slaves to submit their resource needs
#
#   build compound launch parameters using ${instancedir}/${instances} files
#
    for i in ${instances} ; do
       echo "INFO: (master) creating ${instancedir}/${i}.ACK"
       touch ${instancedir}/${i}.ACK                           # acknowledge reception of slave task requirements
    done
    sleep 2
#
#   execute MPI tasks on behalf of slaves
#   -instances -instancedir : used, not passed on
#   -pgm : put after pgm(s) from slaves
#   -args : ignored, cannot be used in this context
#   -npex -npey -processorder -tmpdir -tag -spliteo -debug -nocleanup -geometry : passthrough
#   -outp -errp -preexec -packoutput -offset -mpiargs -dryrun -minstdout : passthrough
#   -ib -noib -mpirun -coremap -nodemap -openmpi -mpich2 -verbose : passthrough
#   all other arguments : ignored
#
    unset PARALLEL_INSTANCES_DIR
    MasterPgm="%%${OMP_NUM_THREADS} ::${npex}x${npey} /./$(pwd -P) ${pgm}"
    [[ "$pgm" == NoNe ]] && MasterPgm=""
    set -x
    ${0} -npex ${pes:-${npex}} -npey 1 -processorder "${processorder}" -tmpdir "${tmpdir}" -minstdout ${minstdout} \
         -coremap "${coremap}" -nodemap "${nodemap}" -ib "${ib}" -noib "${noib}" -verbose "${verbose}" \
         -tag "${tag}" -spliteo "${spliteo}" -debug "${debug}" -nocleanup "${nocleanup}" -geometry "${geometry}" \
         -outp "${outp}" -errp "${errp}" -preexec "${preexec}" -packoutput "${packoutput}" -dryrun "${dryrun}" \
         -offset "${offset}" -mpiargs "${mpiargs}" -selftest "${selftest}" \
         -mpirun "${mpirun}" -openmpi "${openmpi}" -mpich2 "${mpich2}" \
         -pgm $(cd ${instancedir} ; cat ${instances} | xargs) ${MasterPgm}
    StAtUs=$?
    set +x
    echo ""
    for i in ${instances} ; do
       mv ${instancedir}/${i} ${instancedir}/${i}.DONE      # remove slave work requests
#      acknowledge completion of all slave tasks by creating DONE and then deleting ACK
       rm ${instancedir}/${i}.ACK                  # remove acknowledge flag
       echo "INFO: (master) ${instancedir}/${i}.DONE"
    done
    echo "INFO: (master) END of ${slaves} slave(s) '${instances}'"
    exit  ${StAtUs}                                        # master job done
  else                    # a slave instance, there is ONE AND ONLY ONE item  in instances
#
#   -instances -instancedir : used, not passed on
#   -npex -npey -pgm : massaged and passed on to master
#   -args : ignored, cannot be used in this context
#   all other arguments : ignored
#
    (( 1 == $(echo ${pgm} | wc -w) )) && pgm=" ${pgm} 0 1 $((npe_total-1)) @@ "  # transform -pgm name  into -pgm name first increment last
    echo "%%${OMP_NUM_THREADS} /./$(pwd -P) ::${npex}x${npey} ${pgm} " >${instancedir}/${instances}.tmp || exit 1
    mv ${instancedir}/${instances}.tmp ${instancedir}/${instances}  || exit 1
    echo "INFO: START of slave instance ${instances}"
    if wait_for creation ${timeout} ${instancedir} ${instances}.ACK # wait for master to acknowledge resources (ACK created)
    then
      wait_for deletion 2000000000 ${instancedir} ${instances}.ACK  # wait for master to signal job done (infinite timeout)
      wait_for creation ${timeout} ${instancedir} ${instances}.DONE # done is signalled by creating DONE and then deleting ACK
      rm ${instancedir}/${instances}.DONE
      echo "INFO: (slave ${instances}) ${instances}.DONE received"
      echo "INFO: END of slave instance ${instances}"
      exit $?     # slave job done
    else
      echo "ERROR: instance ${instances} never found ${instances}.ACK"
      exit 1
    fi
  fi # master or slave instance
fi  # co-running
#################################################################################################################################
#            end of co-running instances processing
#################################################################################################################################
((MpiCommWorlds=0))  # number of MPI comm worlds
PeInWorld[0]=${npe_total}
ThreadsInWorld[0]=${OMP_NUM_THREADS}
# process -args @file
[[ "${args}" == @* ]] && args2=${args#@} && [[ -f ${args2} ]] && args="$(xargs <${args2})"
# args variable now contains all program arguments (BEWARE: args will be passed to ALL programs)
#
# -pgm @file
# up to 5 items per line (same syntax as -pgm)
# [directory] executable first_pe increment last_pe|@       (all 5 items)
# [directory] executable first_pe increment                 (last pe will be number of PEs - 1)
# [directory] executable +number_of_pes                     (use next number_of_pes PEs)
# -pgm @file will be replaced by   -pgm $(cat file)
#
# -pgm syntax  (@ for last_pe means number of pes - 1)
# -pgm {    [directory] executable first_pe increment last_pe|@  } (repeated)
# -pgm {    [directory] executable +number_of_pes   }  (repeated)
#
#  @@ end of an MPI world
#  ::N   ::NXxNY       PE specification for a world
#  %%nthreads          thread(OpenMP) specification for a world
#  /./dir_path         base directory for a world
#
# process -pgm @file
[[ "${pgm}" == @* ]] && pgm2=${pgm#@} && [[ -f ${pgm2} ]] && pgm="$(xargs <${pgm2})"
# pgm variable now contains all that is needed
#
[[ -n "${debug}" ]] && echo "INFO: debug flag = '${debug}'" && set -x
#################################################################################################################################
# reset TMPDIR to make sure it is visible to all MPI tasks  (ideally on a COHERENT filesystem, NFS BEWARE)
mkdir -p ${tmpdir}
export TMPDIR=${tmpdir}
#################################################################################################################################
# prepare node file (list of node names) and slot_number array (core ordinal on node)
#################################################################################################################################
echo "INFO: Cores per Node = $CoresOnNode"
MY_NODEFILE=$tmpdir/PBS_NODEFILE$$
rm -f ${MY_NODEFILE}

if [[ ! -r ${nodefile} ]] ; then   # no nodefile supplied, make one
  nodefile=${MY_NODEFILE}__
  rm -f ${nodefile}
  if tty -s ; then
    echo "INFO: no nodefile found, creating one with $npe_total x $OMP_NUM_THREADS entries"
    ((r=0))
    while ((r < npe_total*OMP_NUM_THREADS)) ; do echo localhost >>${nodefile} ; ((r=r+1)) ; done
  else
    if [[ "$hostos" == Linux ]] ; then  # no nodefile expected under AIX in batch mode
      echo "ERROR: no nodefile found, not an interactive job, Linux OS, aborting"
      rm -rf ${tmpdir}
      exit 1
    fi
  fi  # tty -s
#else
#  echo "INFO: sorting ${nodefile} into ${MY_NODEFILE}__"
#  sort  <${nodefile} >${MY_NODEFILE}__
#  nodefile=${MY_NODEFILE}__
#  cat ${nodefile}
fi  # no nodefile supplied

((TotalThreads=npe_total*OMP_NUM_THREADS))
if [[ "${coremap}" == NoNe && "${nodemap}" == NoNe ]] ; then
  nhosts=$(uniq ${nodefile} | wc -l | sed -e 's/ //g')   # number of hosts in node file
  ((loops=(npe_total+nhosts-1)/nhosts))   # number of tasks per host (rounded up)
  if ((loops*OMP_NUM_THREADS > MaxCores)) ; then
    echo "ERROR: not enough cores available on node, $((loops*OMP_NUM_THREADS)) requested, $((MaxCores)) available"
    rm -rf ${tmpdir}
    exit 1
  fi
  for i in $(uniq ${nodefile}) ; do
    for j in $(seq $loops) ; do
      echo ${i}
    done
  done | head -${npe_total} >>${MY_NODEFILE}  # one entry per task
fi
if [[ "${coremap}" != NoNe && "${nodemap}" != NoNe ]] ; then
  echo "ERROR: coremap and nodemap are mutually exclusive options"
  exit 1
fi
if [[ "${coremap}" != NoNe ]] ; then
  expand_core_map
fi
if [[ "${nodemap}" != NoNe ]] ; then
  expand_node_map
fi
ListOfNodes=($(cat ${MY_NODEFILE}))
NumberOfNodes=${#ListOfNodes[@]}
#  [[ -n $verbose ]] && echo "INFO: NumberOfHosts = $NumberOfHosts"
#  [[ -n $verbose ]] && echo "INFO: Host list = ${ListOfHosts[@]}"
[[ -n $verbose ]] && echo "INFO: Number or processes = $NumberOfNodes"
[[ -n $verbose ]] && echo "INFO: Node list = ${ListOfNodes[@]}"
if ((NumberOfNodes!=npe_total)) ; then
  echo "ERROR: inconsistent number of processes, requested ${npe_total}, mapped ${NumberOfNodes}"
  rm -rf ${tmpdir}
  exit 1
fi
#################################################################################################################################
# create script anc C executable used in self test if needed
#################################################################################################################################
if [[ -n ${selftest} ]] ; then
  selftest="${tmpdir}/${selftest}"
  pgm="$( echo ${pgm} | sed -e s:PGM:${selftest}:g )"
  cat <<EOT >${selftest}
#!/bin/ksh93
echo "NODE FILE='\${MY_NODEFILE}' RPN_COMM_DOM='\${RPN_COMM_DOM}' arguments='\$@' \$(taskset -c -p \$\$ 2>/dev/null)"
echo "\$(hostname)(\${MP_CHILD}): WORLD_CHILD=\${WORLD_CHILD}, RP_Child=\${RP_Child}, RP_Member=\${RP_Member}, RP_MemberChild=\${RP_MemberChild}, MP_SeqNum=\${MP_SeqNum}, RP_CommWorld=\${RP_CommWorld}, RP_WorldChild=\${RP_WorldChild}"
set -x
if [[ -x ${tmpdir}/mpi_c_test && ${selftest} == *c.* ]] ; then ${tmpdir}/mpi_c_test || exit 1 ; fi
if [[ -x ${tmpdir}/mpi_f_test && ${selftest} == *f.* ]] ; then ${tmpdir}/mpi_f_test || exit 1 ; fi
sleep \${1:-5}
EOT
 chmod 755 ${selftest}
 [[ ${nompi} == run_with_mpi ]] && \
 [[ ${selftest} == *c.* || ${selftest} == *f.* ]] && \
 make_cf_test  # create programs and compile them only if requested and using MPI
fi
#################################################################################################################################
# primary and secondary launching scripts
#################################################################################################################################
export MpiRunScript=${tmpdir}/MpiRunScript_$$       # secondary script(s), one per "MPI" world
export ParallelScript=${tmpdir}/ParallelScript_$$   # primary script(s), one per "MPI" world, will source secondary script(s)
touch ${ParallelScript}.0
[[ ! -r ${ParallelScript}.0 ]] && echo "ERROR: ${tmpdir} not a writable directory" 1>&2 && exit 1
#################################################################################################################################
# stdout and stderr redirection
#################################################################################################################################
export RedirectStdout="1>${tmpdir}/\${MP_SeqNum}/stdout.\${MP_Tag}"   # will be expanded at run time in ParallelScript
export RedirectStderr="2>${tmpdir}/\${MP_SeqNum}/stderr"   # will be expanded at run time in ParallelScript
[[ "${spliteo}" == no ]] && RedirectStderr="2>&1"
[[ -z ${processorder} ]] && RedirectStderr="" && RedirectStdout=""
export  Prefix3="${errp}-"
export  Prefix2="${outp}${errp}-"   # prepare for stderr not split from stdout
[[ "${spliteo}" == yes ]] && Prefix2="${outp}-"
[[ "${tag}" == none    ]] && Prefix2="" && Prefix3=""
[[ "${tag}" == stderr  ]] && Prefix2="" && Prefix3="stderr: "
#################################################################################################################################
# communication variables for RPN_COMM toolkit
#################################################################################################################################
RPN_COMM_DOM=""
RPN_COMM_DIRS="' '"     # RPN_COMM_DIRS no longer used, cd now done in secondary script(s)
SetCurrentDir=$(pwd -P)
set -- $pgm @@
#################################################################################################################################
((ErRoR=0))
((NDomains=0))
((Next=0))
((MpiCommWorld=0))
((npe_total=${PeInWorld[0]}))
((MaxPe=0))
((InstanceOffset=0))
((Instances=0))
((TotalInstances=0))
((TotalThreads=0))
#################################################################################################################################
if [[ "$3" != "" ]] ; then # complex sequence, SPMD or MPMD, (automatic multiple MPI worlds with @@)
#  for i in $(seq 0 1 ${npe_total_m1}) ; do ProGrams[$i]="NoNe" ; Directories[$i]="." ; done
  unset ProGrams Directories
#
  while [[ -n "${1}" ]]
  do
    [[ "${1}" == "@@" && "${2}" == "@@" ]]          && shift && continue   # two @@ flags back to back, eliminate first one
    [[ "${1}" == /./* ]]  && SetCurrentDir=${1#/./} && shift && continue   # used by master from slave to set slave base work directory
    [[ "${1}" == %%* ]]   && NLThreads=${1#%%}      && shift && continue   # from slave to set number of threads per task
    if [[  "${1}" == ::* ]] ; then  # used by master from slave to set number of tasks for this slave
      npe_total=${1#::}
      if [[ ${npe_total} == *x* ]] ; then
        npey=${npe_total#*x}
        npex=${npe_total%x*}
      else
        npey=1
        npex=${npe_total}
      fi
      ((npe_total=npex*npey))
      shift
      continue
    fi
#
    if [[ "${1}" == "@@" ]] ; then  #  wrap up a world
      NLThreads=${NLThreads:-${OMP_NUM_THREADS:-1}}
      PeInWorld[${MpiCommWorld}]=${Instances}
      MembersInWorld[${MpiCommWorld}]=${npey}  # number of members in this world
      ThreadsInWorld[${MpiCommWorld}]=${NLThreads}
      ((TotalThreads=npe_total*NLThreads))
      echo "INFO: MPI world ${MpiCommWorld} will be using ${NLThreads} thread(s) per task"
#     make sure that MaxPe <= npe_total (no overflow) and Instances == MaxPe+1 (no holes)
      ((Instances != MaxPe+1)) && echo "ERROR: holes found, Instances=${Instances}, MaxPe=${MaxPe}" && ((ErRoR=ErRoR+1))
      ((MaxPe > npe_total)) && echo "ERROR: task number overflow, MaxPe(${MaxPe}) > npe_total(${npe_total})" && ((ErRoR=ErRoR+1))
      rm -f ${MpirunScript}.${MpiCommWorld}
      cat <<EOT >${MpiRunScript}.${MpiCommWorld}
#!/bin/ksh93
# SHELL for scripts run under AIX/POE MUST BE /bin/ksh93
export RPN_COMM_DOM='${NDomains}${RPN_COMM_DOM}'
export RPN_COMM_DIRS="${RPN_COMM_DIRS}"        # RPN_COMM_DIRS no longer needed by RPN_COMM_init as cd is now performed by script
export OMP_NUM_THREADS=${NLThreads}
((WORLD_CHILD=MP_CHILD-${InstanceOffset}))
export WORLD_CHILD
[[ -z "${nosep}" ]] &&
  [[ -n "${processorder}" ]] &&
  /usr/bin/printf "============== stdout W:${MpiCommWorld}:%4.4d M:%4.4d-%4.4d Seq:%s ==============\n" \${RP_WorldChild} \${RP_Member} \${RP_MemberChild} \${MP_SeqNum}
EOT
      for i in  $(seq 0 1 $((Instances-1))) ; do
        cat <<EOT >>${MpiRunScript}.${MpiCommWorld}
          if (( MP_CHILD == $((i+InstanceOffset)) ))
          then
            cd ${Directories[$i]}
            ${preexec} ${ProGrams[$i]} ${args}
            Status=\$?
            ((Status==0)) && $c_rm -f ${tmpdir}/fail.\${MP_Tag}.\${MP_SeqNum}
            echo "END of child \${WORLD_CHILD}, status = \${Status}  \$($c_date)"
          fi
EOT
      done
      ((InstanceOffset=TotalInstances))
      [[ -n "${2}" ]] && ((MpiCommWorld=MpiCommWorld+1))   # do not bump world counter if nothing after @@
      ((MpiCommWorlds=MpiCommWorlds+1))
      unset RPN_COMM_DOM RPN_COMM_DIRS ProGrams Directories                    # reset world related variables nd counters
      ((NDomains=0))
      ((Next=0))
      ((MaxPe=0))
      ((Instances=0))
      ((npe_total=0))   # reset ::ntasks for next world
      unset NLThreads
      shift
      continue
    fi  #  if "${1}" == "@@"  wrap up a world
#
    ((NDomains=NDomains+1))
    Temp="${1}"
    [[  "${1}" == /* ]] || Temp="${SetCurrentDir}/${1}"  # relative path, add "current" directory
    Directory="${SetCurrentDir}"
    [[ -d "${Temp}" ]] && Directory="${Temp}" && shift   # $1 was pointing to a directory
#
    Program="$1"
    [[  "${1}" == /* ]] || Program="${SetCurrentDir}/${1}"  # relative path, add "current" directory
    shift
    if [[ !  -x "$Program" ]] ; then echo program $Program does not exist or is not executable ; ((ErRoR=ErRoR+1)) ; fi
#
    temp=${1}
    [[ "${temp}" == @@ ]] && temp="+${npe_total}"  # replace @@ with +npe_total
    if [[ $temp = +* ]] ; then
       temp=${temp#+}
       ((First=Next))
       ((Increment=1))
       ((Last=First+temp-1))
       ((Next=Last+1))
       [[ "${1}" != @@ ]] && shift   # do not shift @@
    else
      ((First=${1}))
      ((Increment=${2}))
      Last=${3}
      if [[ "${Last}" == @ ]] ; then ((Last=npe_total-1)) ; fi
      shift ; shift ; shift
    fi
    RPN_COMM_DOM="$RPN_COMM_DOM,${First},${Increment},${Last}"
#    RPN_COMM_DIRS="$RPN_COMM_DIRS,'${Directory}'"
    RPN_COMM_DIRS="$RPN_COMM_DIRS,'.'"  # no longer needed, we do the cd ..... in the script, so every directory becomes . :-)
    ((LocalInstances=0))
    for i in $(seq ${First} ${Increment} ${Last} )
    do
      ((Instances=Instances+1))
      ((LocalInstances=LocalInstances+1))
      ((TotalInstances=TotalInstances+1))
      ((TotalInstances>TotalTasks)) && echo "ERROR: too many tasks requested (${TotalInstances}), only ${TotalTasks} available" && ((ErRoR=ErRoR+1))
      if [[ -n "${ProGrams[$i]}" ]] ; then
        echo ERROR: duplicate program assignment "${ProGrams[$i]}" vs "$Program" in slot $i
        ((ErRoR=ErRoR+1))
      else
        ProGrams[$i]="$Program"
        Directories[$i]=$Directory
        ((i>MaxPe)) && ((MaxPe=i))
      fi
    done
    echo "INFO: ${LocalInstances} instances of '$Program' ( $((InstanceOffset+First)) to $((InstanceOffset+Last)) by ${Increment} ) in MPI world ${MpiCommWorld}"
  done # while [[ "$1" != "" ]]
#
  if [[ "$ErRoR" != "0" ]] ; then echo "$ErRoR ERROR(S) detected" ; local_cleanup ; exit 1 ; fi
#
else #################### simple SPMD case, one MPI world, old style call ##########################
#
  ((TotalInstances=npe_total))
  if [[ ! -x $pgm ]] ; then                        # same executable for all worlds
    echo "ERROR: $pgm does not exist or is not executable"
    exit 1
  fi
  MpiCommWorld=0
  MpiCommWorlds=1
  ((NDomains=NDomains+1))
  RPN_COMM_DOM=",0,1,$((npe_total-1))"
  PeInWorld[0]=${npe_total}
  ThreadsInWorld[0]=${OMP_NUM_THREADS}
  MembersInWorld[0]=${npey}
  ((TotalThreads=OMP_NUM_THREADS))
  cat <<EOT >${MpiRunScript}.${MpiCommWorld}
#!/bin/ksh93
# SHELL for scripts run under AIX/POE MUST BE /bin/ksh93
unset RPN_COMM_DOM
unset RPN_COMM_DIRS
export OMP_NUM_THREADS=${NLThreads:-${OMP_NUM_THREADS}}
((WORLD_CHILD=MP_CHILD))
export WORLD_CHILD
[[ -z "${nosep}" ]] && \
  [[ -n "${processorder}" ]] && \
  /usr/bin/printf "============== stdout W:${MpiCommWorld}:%4.4d M:%4.4d-%4.4d Seq:%s ==============\n" \${RP_WorldChild} \${RP_Member} \${RP_MemberChild} \${MP_SeqNum}
EOT
  cat <<EOT >>${MpiRunScript}.0   # MpiCommWorld is 0 in this case
    ${preexec} ${pgm} ${args}
    Status=\$?
    ((Status==0)) && $c_rm -f ${tmpdir}/fail.\${MP_Tag}.\${MP_SeqNum}
    echo "END of child \${WORLD_CHILD} with status = \${Status}  \$($c_date)"
EOT
  echo "INFO: ${npe_total} instances of '$pgm' in MPI world 0"
#
fi ########################## if complex sequence, possibly MPMD ####################
#################################################################################################################################
# prepare primary launch scripts (one per world)
#################################################################################################################################
MpiCommWorld=0
export WorldOffset=0
#
# prepare tagging format and contents
#
#tag_values='${RP_CommWorld} ${RP_Member} ${RP_MemberChild}'
#tag_format='%1d-%4.4d-%4.4d'
[[ "${tag}" == full ]] && ((MpiCommWorlds>1)) && F_world='%1d-' && V_world='${RP_CommWorld}'
[[ "${tag}" == full ]] && F_member='%4.4d-' && V_member='${RP_Member}' && \
                          F_child='%4.4d' && V_child='${RP_MemberChild}'
[[ "${tag}" == member ]] && F_member='%4.4d-' && V_member='${RP_Member}'
[[ "${tag}" == child ]]  && F_child='%4.4d' && V_child='${RP_MemberChild}'
[[ "${tag}" == seq ]]    && F_seq='%4.4d' && V_seq='${MP_SeqNum}'
export F_world V_world F_member V_member F_child V_child F_seq V_seq

#################################################################################################################################
while ((MpiCommWorld<MpiCommWorlds))
do
# ${PeInWorld[${MpiCommWorld}]} ${MembersInWorld[${MpiCommWorld}]} $WorldOffset
  ((npex=${PeInWorld[${MpiCommWorld}]}/${MembersInWorld[${MpiCommWorld}]}))
  cat <<EOT >${ParallelScript}.${MpiCommWorld}
#!/bin/ksh93
ulimit -s unlimited
[[ -n "${debug}" ]] && set -x
#
export MP_CHILD=\${MP_CHILD:-\${PMI_RANK:-\${OMPI_COMM_WORLD_RANK}}}  # poe/mpich2/openmpi
typeset -Z4 MP_SeqNum    # NOT VALID FOR bash, ksh family only
((MP_CHILD=MP_CHILD+ChildOffset))   # need to add ChildOffset for this MPI world to MP_CHILD (always 0 for poe)
export RP_WorldChild
((RP_WorldChild=MP_CHILD-${WorldOffset}))   # child ordinal in this world
export RP_CommWorld=${MpiCommWorld}         # ordinal of this MPI world
export MP_SeqNum="\$((MP_CHILD))"           # global sequence number
export RP_Child="\$((MP_CHILD))"            # global child number, same as MP_CHILD
export RP_Member=\$((RP_WorldChild/${npex}+${offset}))      # member number in this world (including offset)
export RP_MemberChild=\$((RP_WorldChild-RP_WorldChild/${npex}*${npex}))   # child ordinal within this member
MP_Tag="\$(/usr/bin/printf '${F_world}${F_member}${F_child}${F_seq}' ${V_world} ${V_member} ${V_child} ${V_seq})"
[[ -n "${debug}" ]] && $c_env | $c_sort >${tmpdir}/.mpi_env_\$\$_\$($c_hostname)
#
#[[ -n "${processorder}" ]] &&
$c_touch ${tmpdir}/fail.\${MP_Tag}.\${MP_SeqNum}
. ${MpiRunScript}.${MpiCommWorld}  ${RedirectStdout} ${RedirectStderr}
[[ -x ${tmpdir}/null_mpi ]] && ${tmpdir}/null_mpi
[[ -n "${processorder}" ]] || exit 0     # no listings to process, exit
lines=0
the_file=${tmpdir}/\${MP_SeqNum}/stdout.\${MP_Tag}
[[ -r \${the_file} ]] && lines="\$($c_wc -l <\${the_file})" && ((lines<${minstdout})) && $c_rm \${the_file}  # remove stdout files shorter than minimum
[[ \${MP_SeqNum} != *00 ]] && $c_true && exit
$c_sleep 3
for the_file in ${tmpdir}/\${MP_SeqNum%??}[0-9][0-9]/stdout.*
do
  MP_Tag=\${the_file##*.}
  [[ -f \${the_file} ]] && { [[ -z "${nosep}" ]] && echo "" ; $c_cat \${the_file} | $c_sed "s/^/${Prefix2}\${MP_Tag}: /" ;} >>${tmpdir}/\${MP_SeqNum}/stdout
  [[ -f \${the_file%/*}/stderr ]] && { [[ -z "${nosep}" ]] && echo "" ; [[ -z "${nosep}" ]] && echo "${Prefix3}\${MP_Tag}: ============== stderr \${MP_Tag} ==============" ; $c_cat \${the_file%/*}/stderr | $c_sed "s/^/${Prefix3}\${MP_Tag}: /" ; } >>${tmpdir}/\${MP_SeqNum}/stdout
done
true
EOT
#
  chmod 755 ${ParallelScript}.${MpiCommWorld}
  ((WorldOffset=WorldOffset+${PeInWorld[${MpiCommWorld}]}))
  ((MpiCommWorld=MpiCommWorld+1))
done    # while ((MpiCommWorld<MpiCommWorlds))
#################################################################################################################################
# we are now almost ready to launch
#
((npe_total=TotalInstances))
# create stdout/stderr directories if necessary
if [[ -n ${processorder} ]] ; then
  for ISeqNum in $(seq 0 1 $((TotalInstances-1)) )    #  a arranger pour le cas MPMD ====================================
  do
    /usr/bin/printf "%s/%4.4d\n" ${tmpdir} ${ISeqNum}
  done  | xargs -l30 mkdir
#  echo "INFO: =============================================="
#  echo "INFO: temporary listings for all members in ${tmpdir}"
#  echo "INFO: =============================================="
fi
echo "INFO: START of parallel execution : $(date)"
#
if [[ ${nompi} == run_with_mpi ]] ; then  # MPI launch (linux or AIX supported)
#
  SystemPlatform=$(uname -s)
  MpiImplementation=""
  [[ -n ${openmpi} ]] && MpiImplementation="_openmpi"  # openmpi MPI implementation
  [[ -n ${mpich2} ]] && MpiImplementation="_mpich2"                         # mpich2 MPI implementation
  echo "INFO: ${MPI_EXEC:-mpiexec_${SystemPlatform}${MpiImplementation}} ${mpiargs}"
  ${MPI_EXEC:-mpiexec_${SystemPlatform}${MpiImplementation}} ${mpiargs}
#
else   # background launch (it is assumed that there is only one world)
#
  ((MP_CHILD=0)) # MP_CHILD used to indicate logical child number (like MPI case)
  export MP_CHILD
  while ((MP_CHILD<TotalInstances))
  do
    ${ParallelScript}.0 &    # world no 0
    ((MP_CHILD=MP_CHILD+1))
  done
  echo "INFO: waiting for ${MP_CHILD} background task(s) to terminate"
  wait
#
fi
echo "INFO: END of parallel execution : $(date)"
#################################################################################################################################
# post process/order listings if required
#
[[ "${e}" == YES ]] && ListFailed
#
if [[ -n ${packoutput} && -n ${processorder} ]] ; then   # post process stderr/stdout from all processes
  print_separator " start of parallel run "
  ( cd ${tmpdir} ; ${packoutput} [0-9]*[0-9] )
  print_separator " end of parallel run "
  echo "INFO: END of listing processing : $(date)"
fi
#
local_cleanup  # exit status set here 0:OK 1:processes failed 2:tmpdir removal failed

